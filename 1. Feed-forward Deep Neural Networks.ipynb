{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules:\n",
    "---------------\n",
    "We import matplotlib to plot some data, urllib and os to collect the data, and numpy, theano, lasagne, nolearn and  scikit-learn for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "\n",
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import visualize\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST Dataset:\n",
    "--------------\n",
    "The MNIST dataset is the common first dataset used as a demo in deep learning. More specifically, it's a digit classification learning model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    with gzip.open(\"datasets/mnist/mnist.pkl.gz\") as fin:\n",
    "        data = pickle.load(fin)\n",
    "\n",
    "    X_train, y_train = data[0]\n",
    "    X_val, y_val = data[1]\n",
    "    X_test, y_test = data[2]\n",
    "\n",
    "    X_train = X_train.reshape((-1, 1, 28, 28))\n",
    "    X_val = X_val.reshape((-1, 1, 28, 28))\n",
    "    X_test = X_test.reshape((-1, 1, 28, 28))\n",
    "\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_val = y_val.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution Networks: \n",
    "--------------------\n",
    "\n",
    "The following code defines our convolution network and trains it using a GPU/CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[('input', layers.InputLayer),\n",
    "            ('conv2d1', layers.Conv2DLayer),\n",
    "            ('maxpool1', layers.MaxPool2DLayer),\n",
    "            ('conv2d2', layers.Conv2DLayer),\n",
    "            ('maxpool2', layers.MaxPool2DLayer),\n",
    "            ('dropout1', layers.DropoutLayer),\n",
    "            ('dense', layers.DenseLayer),\n",
    "            ('dropout2', layers.DropoutLayer),\n",
    "            ('output', layers.DenseLayer),\n",
    "            ],\n",
    "    # input layer\n",
    "    input_shape=(None, 1, 28, 28),\n",
    "    # layer conv2d1\n",
    "    conv2d1_num_filters=16,\n",
    "    conv2d1_filter_size=(6, 6),\n",
    "    conv2d1_nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    conv2d1_W=lasagne.init.GlorotUniform(),  \n",
    "    # layer maxpool1\n",
    "    maxpool1_pool_size=(2, 2),    \n",
    "    # layer conv2d2\n",
    "    conv2d2_num_filters=16,\n",
    "    conv2d2_filter_size=(5, 5),\n",
    "    conv2d2_nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    # layer maxpool2\n",
    "    maxpool2_pool_size=(2, 2),\n",
    "    # dropout1\n",
    "    dropout1_p=0.5,    \n",
    "    # dense\n",
    "    dense_num_units=192,\n",
    "    dense_nonlinearity=lasagne.nonlinearities.rectify,    \n",
    "    # dropout2\n",
    "    dropout2_p=0.5,    \n",
    "    # output\n",
    "    output_nonlinearity=lasagne.nonlinearities.softmax,\n",
    "    output_num_units=10,\n",
    "    # optimization method params\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.9,\n",
    "    max_epochs=8,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Train the network\n",
    "nn = net1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction:\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = net1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano & Feature Extraction:\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_layer = layers.get_output(net1.layers_['dense'], deterministic=True)\n",
    "output_layer = layers.get_output(net1.layers_['output'], deterministic=True)\n",
    "input_var = net1.layers_['input'].input_var\n",
    "\n",
    "f_output = theano.function([input_var], output_layer)\n",
    "f_dense = theano.function([input_var], dense_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
